---
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DDModeling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE, eval= TRUE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

One, if not the central problem in fitting drift-diffusion models is given by their random nature, namely their diffusion. Thus, no functional relation between the domain (i.e. the model parameters) and the codomain (i.e. the data) can be established. However, this does not inherently predict that there is no pattern in the connection between the two. One way to address such problems is through deep learning. However, such an endeavor would require a lot of data. Fortunately, DDModeling has got you covered!

You can easily convert GRIDs into learning data (see [here](Generate_GRIDs.html) for a tutorial on GRID generation).

```{r eval=FALSE}
library(DDModeling)
DSTP <- DDModel(model="DSTP",task = "flanker", CDF_perc = c(0.1,0.3,0.5,0.7,0.9), CAF_perc = c(0.0,0.2,0.4,0.6,0.8,1.0))
Grid_DSTP <- Generate_GRID(model = DSTP, path = getwd(), name = "DSTP_Flanker", eval_pts = rep(2,ncol(DSTP@DM)))
DATA <- Import_GRID(grid_path = Grid_DSTP,to = "keras_data")
```

The function `Import_GRID()` needs two arguments **grid path** and **to**. The first takes the path to the directory where the GRID is stored, the second specifies the format in which the import should be done. `to = "data_keras"` returns a list containing an INPUT and an OUTPUT which can be used for deep learning.

DDModeling uses Tensorflow and Keras for deep learning, so make sure you install them before!

With the data setup you can quickly create a model!

```{r eval=FALSE}
# General note: You should allways scale INPUT and OUTPUT!
DATA$INPUT <- scale(DATA$INPUT)
DATA$OUTPUT <- scale(DATA$OUPUT)
# DDModeling give you a handy function to extracted the scaling information!
DATA_scale <- Scale_DL_Data(data = DATA)

library(keras)
model <- keras_model_sequential() %>%
  layer_dense(input_shape=ncol(DATA$INPUT),units = 10, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 20, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = ncol(DATA$OUTPUT),activation = "linear")

model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = list("mae","mape")
)

model %>% fit(
    x = DATA$INPUT, y = DATA$OUTPUT,
    epochs = 50,
    batch_size = 16,
    verbose = 1,
    validation_split = 0.1,
    shuffle = TRUE,
    callbacks = list(
      callback_reduce_lr_on_plateau(factor = 0.05)
    )
)
```

You can either use this in your own code in any way you like or further integrate it for customization with DDModeling!

